# Affiel_Going-Deeper_NLP-study
|No|프로젝트 주제|핵심이론|
|---|---|---|
|1-1|텍스트 데이터 다루기|다양한 텍스트 데이터 전처리 기법과 Word나 형태소 레벨의 tokenizer 및 subword 레벨 tokenizing 기법(BPE, sentencepiece) 학습|
|1-2|맛진 단어사전 만들기|단어사전을 만들어보고 이를 토대로 perplexity를 측정해보는 프로젝트|
|2-1|텍스트의 분포로 벡터화하기|텍스트 분포를 이용한 텍스트의 벡터화 방법들(BoW, DTM, TF-IDF, LSA, LDA)|
|2-2|뉴스 카테고리 다중분류|뉴스 텍스트의 주제를 분류하는 task를 다양한 기법으로 시도해보고 비교, 분석|
|3-1|워드 임베딩|워드 임베딩 벡터(Word2Vec, FastText, Glove)의 원리와 사용법을 학습|
|3-2|임베딩 내 편향성 알아보기|데이터의 편향성을 알아보기 위한 WEAT 기법을 이해하고, 워드 임베딩에 직접 적용|
|4-1|Seq2seq와 Attention|언어 모델이 발전해 온 과정에 대해 배우고, Seq2seq에 대해 학습|
|4-2|Seq2seq로 번역기 만들기|Attention 기법을 추가하여 Seq2seq 기반의 번역기 성능을 높여보기|
|5-1|Transformer가 나오기까지|Attention 복습 및 트랜스포머에 포함된 모듈을 심층적으로 이해하는 단계|
|5-2|Transformer로 번역기 만들기|트랜스포머를 이용해 번역기를 만드는 프로젝트|
|6-1|기계 번역이 걸어온 길|자연어 처리에서 Data Augmentation 방법과, 성능 측정 방법 학습|
|6-2|번역가는 대화에도 능하다|다양한 디코딩 방식을 활용해 모델 구현 후 BLEU Score를 이용하여 성능 평가, 한국어 챗봇 구현 프로젝트|
|7-1|modern NLP의 흐름에 올라타보자기|트랜스포머를 바탕으로 한 최근 NLP 모델에 대해 학습|
|7-2|BERT pretrained model 제작|대표적인 pretrained language model인 BERT 원리에 대해 학습|
|8-1|NLP Framework의 활용|다양한 NLP Framework들과 Huggingface transformer를 중심으로 설계구조와 활용법 학습|
|8-2|HuggingFace 커스텀 프로젝트 만들기|Huggingface transformer를 활용한 커스텀 프로젝트 수행|
