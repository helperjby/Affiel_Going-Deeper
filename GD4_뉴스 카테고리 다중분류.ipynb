{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GD-4_프로젝트: Vocabulary Size를 변경해서 시도해보기\n",
    "\n",
    "지금까지는 모델을 변경하고, 모델을 조합해서 성능을 올리는 일에 힘썼습니다. 그런데 어쩌면 성능을 높이는 방법은 단순히 모델을 조정하는 일에 한정되지 않을 수 있습니다. 데이터의 전처리는 모델의 성능에 영향을 직접적으로 줍니다. 특히나 Bag of Words를 기반으로 하는 DTM이나 TF-IDF의 경우, 사용하는 단어의 수를 어떻게 결정하느냐에 따라서 성능에 영향을 줄 수 있겠죠.\n",
    "\n",
    "중요도가 낮은 단어들까지 포함해 너무 많은 단어를 사용하는 경우에도 성능이 저하될 수 있고, 반대로 너무 적은 단어들을 사용해도 성능이 저하될 수 있습니다. 이렇게 변화된 단어의 수는 또 어떤 모델을 사용하느냐에 따라 유리할 수도, 불리할 수도 있습니다.\n",
    "\n",
    "단어의 수에 따라서 모델의 성능이 어떻게 변하는지 테스트해 봅시다.\n",
    "\n",
    "\n",
    "```(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=10000, test_split=0.2)```\n",
    "\n",
    "앞서 `num_words`로 사용할 단어의 수를 조정할 수 있다는 것을 배웠습니다. 빈도수가 많은 순서대로 나열했을 때, `num_words`의 인자로 준 정숫값만큼의 단어를 사용하고 나머지 단어는 전부 `<unk>`로 처리하는 원리였었죠.\n",
    "\n",
    "아래의 두 가지 경우에 대해서 지금까지 사용했던 모델들의 정확도를 직접 확인해 보세요.\n",
    "\n",
    "<br/>\n",
    "\n",
    "라이브러리 버전을 확인해 봅니다\n",
    "***\n",
    "사용할 라이브러리 버전을 둘러봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 버전을 확인해 봅니다\n",
    "***\n",
    "사용할 라이브러리 버전을 둘러봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "3.5.1\n",
      "0.11.2\n",
      "1.19.2\n",
      "1.3.5\n",
      "1.0.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib as plt\n",
    "import seaborn as sea\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from tensorflow.keras.datasets import reuters\n",
    "\n",
    "print(tf.__version__)\n",
    "print(plt.__version__)\n",
    "print(sea.__version__)\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vocab size 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 모든 단어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JBY\\anaconda3\\envs\\3.7\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\JBY\\anaconda3\\envs\\3.7\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 상위 10,000개 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_10000, y_train_10000), (x_test_10000, y_test_10000) = reuters.load_data(num_words=10000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 상위 5,000개 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_5000, y_train_5000), (x_test_5000, y_test_5000) = reuters.load_data(num_words=5000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 복원하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 all vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = { index+3 : word for word, index in word_index.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "  index_to_word[index]=token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n"
     ]
    }
   ],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train = decoded\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246\n"
     ]
    }
   ],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_test)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test = decoded\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 vocab 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_10000 = reuters.get_word_index(path=\"reuters_word_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word_10000 = { index+3 : word for word, index in word_index_10000.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "  index_to_word_10000[index]=token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n"
     ]
    }
   ],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train_10000)):\n",
    "    t = ' '.join([index_to_word_10000[index] for index in x_train_10000[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train_10000 = decoded\n",
    "print(len(x_train_10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246\n"
     ]
    }
   ],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_test_10000)):\n",
    "    t = ' '.join([index_to_word_10000[index] for index in x_test_10000[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test_10000 = decoded\n",
    "print(len(x_test_10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 vocab 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_5000 = reuters.get_word_index(path=\"reuters_word_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word_5000 = { index+3 : word for word, index in word_index_5000.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "  index_to_word_5000[index]=token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n"
     ]
    }
   ],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_train_5000)):\n",
    "    t = ' '.join([index_to_word_5000[index] for index in x_train_5000[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_train_5000 = decoded\n",
    "print(len(x_train_5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246\n"
     ]
    }
   ],
   "source": [
    "decoded = []\n",
    "for i in range(len(x_test_5000)):\n",
    "    t = ' '.join([index_to_word_5000[index] for index in x_test_5000[i]])\n",
    "    decoded.append(t)\n",
    "\n",
    "x_test_5000 = decoded\n",
    "print(len(x_test_5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 벡터화 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 26506)\n"
     ]
    }
   ],
   "source": [
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "print(x_train_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 26506)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "print(tfidfv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 9670)\n"
     ]
    }
   ],
   "source": [
    "dtmvector_10000 = CountVectorizer()\n",
    "x_train_dtm_10000 = dtmvector_10000.fit_transform(x_train_10000)\n",
    "print(x_train_dtm_10000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 9670)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer_10000 = TfidfTransformer()\n",
    "tfidfv_10000 = tfidf_transformer_10000.fit_transform(x_train_dtm_10000)\n",
    "print(tfidfv_10000.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 4867)\n"
     ]
    }
   ],
   "source": [
    "dtmvector_5000 = CountVectorizer()\n",
    "x_train_dtm_5000 = dtmvector_5000.fit_transform(x_train_5000)\n",
    "print(x_train_dtm_5000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 4867)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer_5000 = TfidfTransformer()\n",
    "tfidfv_5000 = tfidf_transformer_5000.fit_transform(x_train_dtm_5000)\n",
    "print(tfidfv_5000.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 직접 단어 개수를 설정해서 사용\n",
    "***\n",
    "위 단계에서 5000으로 제시된 num_words를 다양하게 바꾸어 가며 성능을 확인해보세요. 변화된 단어 수에 따른 모델의 성능을 연구해 보세요. 최소 3가지 경우 이상을 실험해 보기를 권합니다.\n",
    ">사용할 모델\n",
    ">\n",
    ">나이브 베이즈 분류기, CNB, 로지스틱 회귀, 서포트 벡터 머신, 결정 트리, 랜덤 포레스트, 그래디언트 부스팅 트리, 보팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 나이브 베이즈 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score #정확도 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.5997328584149599\n"
     ]
    }
   ],
   "source": [
    "x_test_dtm = dtmvector.transform(x_test) #테스트 데이터를 DTM으로 변환\n",
    "tfidfv_test = tfidf_transformer.transform(x_test_dtm) #DTM을 TF-IDF 행렬로 변환\n",
    "\n",
    "predicted = model.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_10000 = MultinomialNB()\n",
    "model_10000.fit(tfidfv_10000, y_train_10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6567230632235085\n"
     ]
    }
   ],
   "source": [
    "x_test_dtm_10000 = dtmvector_10000.transform(x_test_10000) #테스트 데이터를 DTM으로 변환\n",
    "tfidfv_test_10000 = tfidf_transformer_10000.transform(x_test_dtm_10000) #DTM을 TF-IDF 행렬로 변환\n",
    "\n",
    "predicted_10000 = model_10000.predict(tfidfv_test_10000) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test_10000, predicted_10000)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5000 = MultinomialNB()\n",
    "model_5000.fit(tfidfv_5000, y_train_5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6731967943009796\n"
     ]
    }
   ],
   "source": [
    "x_test_dtm_5000 = dtmvector_5000.transform(x_test_5000) #테스트 데이터를 DTM으로 변환\n",
    "tfidfv_test_5000 = tfidf_transformer_5000.transform(x_test_dtm_5000) #DTM을 TF-IDF 행렬로 변환\n",
    "\n",
    "predicted_5000 = model_5000.predict(tfidfv_test_5000) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test_5000, predicted_5000)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 컴플리먼트 나이브베이즈 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComplementNB()"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb = ComplementNB()\n",
    "cb.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7649154051647373\n"
     ]
    }
   ],
   "source": [
    "predicted = cb.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.50      0.63        12\n",
      "           1       0.63      0.88      0.73       105\n",
      "           2       0.91      0.50      0.65        20\n",
      "           3       0.87      0.91      0.89       813\n",
      "           4       0.75      0.93      0.83       474\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.92      0.86      0.89        14\n",
      "           7       1.00      0.67      0.80         3\n",
      "           8       0.43      0.08      0.13        38\n",
      "           9       0.81      0.88      0.85        25\n",
      "          10       0.96      0.73      0.83        30\n",
      "          11       0.55      0.67      0.61        83\n",
      "          12       0.00      0.00      0.00        13\n",
      "          13       0.62      0.54      0.58        37\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.50      0.11      0.18         9\n",
      "          16       0.67      0.77      0.71        99\n",
      "          17       0.00      0.00      0.00        12\n",
      "          18       0.65      0.55      0.59        20\n",
      "          19       0.55      0.80      0.65       133\n",
      "          20       0.89      0.23      0.36        70\n",
      "          21       0.84      0.59      0.70        27\n",
      "          22       0.00      0.00      0.00         7\n",
      "          23       0.71      0.42      0.53        12\n",
      "          24       0.50      0.11      0.17        19\n",
      "          25       0.83      0.61      0.70        31\n",
      "          26       1.00      0.88      0.93         8\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.33      0.10      0.15        10\n",
      "          29       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00        12\n",
      "          31       1.00      0.31      0.47        13\n",
      "          32       1.00      0.80      0.89        10\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      0.71      0.83         7\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.00      0.00      0.00        11\n",
      "          37       0.00      0.00      0.00         2\n",
      "          38       1.00      0.33      0.50         3\n",
      "          39       1.00      0.20      0.33         5\n",
      "          40       0.00      0.00      0.00        10\n",
      "          41       0.67      0.25      0.36         8\n",
      "          42       1.00      0.33      0.50         3\n",
      "          43       1.00      0.17      0.29         6\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.76      2246\n",
      "   macro avg       0.62      0.42      0.46      2246\n",
      "weighted avg       0.75      0.76      0.73      2246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, cb.predict(tfidfv_test), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComplementNB()"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_10000 = ComplementNB()\n",
    "cb_10000.fit(tfidfv_10000, y_train_10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7707034728406055\n"
     ]
    }
   ],
   "source": [
    "predicted_10000 = cb_10000.predict(tfidfv_test_10000) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test_10000, predicted_10000)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComplementNB()"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_5000 = ComplementNB()\n",
    "cb_5000.fit(tfidfv_5000, y_train_5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7707034728406055\n"
     ]
    }
   ],
   "source": [
    "predicted_5000 = cb_5000.predict(tfidfv_test_5000) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test_5000, predicted_5000)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JBY\\anaconda3\\envs\\3.7\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1000, dual=False, max_iter=500, penalty='l1')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "lsvc.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7751558325912734\n"
     ]
    }
   ],
   "source": [
    "predicted = lsvc.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JBY\\anaconda3\\envs\\3.7\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1000, dual=False, max_iter=500, penalty='l1')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "lsvc.fit(tfidfv_10000, y_train_10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7773820124666073\n"
     ]
    }
   ],
   "source": [
    "predicted = lsvc.predict(tfidfv_test_10000) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test_10000, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JBY\\anaconda3\\envs\\3.7\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1000, dual=False, max_iter=500, penalty='l1')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\n",
    "lsvc.fit(tfidfv_5000, y_train_5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7653606411398041\n"
     ]
    }
   ],
   "source": [
    "predicted = lsvc.predict(tfidfv_test_5000) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test_5000, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 결정트리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=9, random_state=0)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=9, random_state=0)\n",
    "tree.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6148708815672307\n"
     ]
    }
   ],
   "source": [
    "predicted = tree.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=9, random_state=0)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=9, random_state=0)\n",
    "tree.fit(tfidfv_10000, y_train_10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6095280498664292\n"
     ]
    }
   ],
   "source": [
    "predicted = tree.predict(tfidfv_test_10000) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test_10000, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=9, random_state=0)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=9, random_state=0)\n",
    "tree.fit(tfidfv_5000, y_train_5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6095280498664292\n"
     ]
    }
   ],
   "source": [
    "predicted = tree.predict(tfidfv_test_5000) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test_5000, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 랜덤 포레스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=5, random_state=0)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.6544968833481746\n"
     ]
    }
   ],
   "source": [
    "predicted = forest.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=5, random_state=0)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest.fit(tfidfv_10000, y_train_10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.674087266251113\n"
     ]
    }
   ],
   "source": [
    "predicted = forest.predict(tfidfv_test_10000) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test_10000, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=5, random_state=0)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "forest.fit(tfidfv_5000, y_train_5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.701246660730187\n"
     ]
    }
   ],
   "source": [
    "predicted = forest.predict(tfidfv_test_5000) #테스트 데이터에 대한 예측\n",
    "print(\"정확도:\", accuracy_score(y_test_5000, predicted)) #예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 딥러닝 모델과 비교해 보기\n",
    "***\n",
    "위 과정을 통해 나온 최적의 모델과 단어 수 조건에서, 본인이 선택한 다른 모델을 적용한 결과와 비교해 봅시다. 감정 분석 등에 사용했던 RNN이나 1-D CNN 등의 딥러닝 모델 중 하나를 선택해서 오늘 사용했던 데이터셋을 학습해 보고 나오는 결과를 비교해 봅시다. 단, 공정한 비교를 위해 이때 Word2Vec 등의 pretrained model은 사용하지 않도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#LSTM 레이어 사용\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JBY\\anaconda3\\envs\\3.7\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\JBY\\anaconda3\\envs\\3.7\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=10000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=145\n",
    "X_train = pad_sequences(x_train, maxlen=max_len)\n",
    "X_test = pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         1280000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46)                5934      \n",
      "=================================================================\n",
      "Total params: 1,417,518\n",
      "Trainable params: 1,417,518\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000 #사용할 단어사전 크기\n",
    "embedding_dim = 128 #임베딩레이어 차원 수 \n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim))\n",
    "model.add(tf.keras.layers.LSTM(128)) \n",
    "model.add(tf.keras.layers.Dense(46, activation='softmax')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "15/15 [==============================] - 10s 298ms/step - loss: 3.6314 - accuracy: 0.2844 - val_loss: 2.5455 - val_accuracy: 0.3450\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.54551, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "15/15 [==============================] - 3s 202ms/step - loss: 2.4764 - accuracy: 0.3520 - val_loss: 2.4206 - val_accuracy: 0.3450\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.54551 to 2.42062, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 2.4074 - accuracy: 0.3527 - val_loss: 2.4075 - val_accuracy: 0.3450\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.42062 to 2.40755, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 2.3890 - accuracy: 0.3523 - val_loss: 2.3198 - val_accuracy: 0.3756\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.40755 to 2.31980, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 2.2848 - accuracy: 0.4230 - val_loss: 2.2630 - val_accuracy: 0.4663\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.31980 to 2.26296, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "15/15 [==============================] - 1s 57ms/step - loss: 2.1981 - accuracy: 0.4789 - val_loss: 2.1012 - val_accuracy: 0.4780\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.26296 to 2.10123, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "15/15 [==============================] - 1s 58ms/step - loss: 2.0710 - accuracy: 0.4910 - val_loss: 2.0478 - val_accuracy: 0.4847\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.10123 to 2.04779, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 2.0248 - accuracy: 0.5039 - val_loss: 2.0063 - val_accuracy: 0.5058\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.04779 to 2.00627, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.9778 - accuracy: 0.5140 - val_loss: 1.9753 - val_accuracy: 0.5181\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.00627 to 1.97535, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.9086 - accuracy: 0.5309 - val_loss: 1.9540 - val_accuracy: 0.5131\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.97535 to 1.95404, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.8573 - accuracy: 0.5410 - val_loss: 1.9086 - val_accuracy: 0.5053\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.95404 to 1.90863, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 1.7808 - accuracy: 0.5555 - val_loss: 1.8329 - val_accuracy: 0.5270\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.90863 to 1.83287, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.6119 - accuracy: 0.5845 - val_loss: 1.7344 - val_accuracy: 0.5543\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.83287 to 1.73436, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50\n",
      "15/15 [==============================] - 1s 55ms/step - loss: 1.5116 - accuracy: 0.6239 - val_loss: 1.6440 - val_accuracy: 0.5943\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.73436 to 1.64404, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "15/15 [==============================] - 1s 56ms/step - loss: 1.4069 - accuracy: 0.6457 - val_loss: 1.7039 - val_accuracy: 0.5810\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.64404\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.3503 - accuracy: 0.6632 - val_loss: 1.5637 - val_accuracy: 0.6199\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.64404 to 1.56371, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n",
      "15/15 [==============================] - 1s 55ms/step - loss: 1.2605 - accuracy: 0.6771 - val_loss: 1.6075 - val_accuracy: 0.6021\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.56371\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.2581 - accuracy: 0.6678 - val_loss: 1.5836 - val_accuracy: 0.6066\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.56371\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.1493 - accuracy: 0.6941 - val_loss: 1.6333 - val_accuracy: 0.6043\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.56371\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 1.0877 - accuracy: 0.7190 - val_loss: 1.5579 - val_accuracy: 0.6149\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.56371 to 1.55794, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 1.0135 - accuracy: 0.7430 - val_loss: 1.5475 - val_accuracy: 0.6316\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.55794 to 1.54747, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.9106 - accuracy: 0.7715 - val_loss: 1.5793 - val_accuracy: 0.6205\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.54747\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.9460 - accuracy: 0.7693 - val_loss: 1.5457 - val_accuracy: 0.6377\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.54747 to 1.54574, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.8261 - accuracy: 0.7918 - val_loss: 1.5444 - val_accuracy: 0.6472\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.54574 to 1.54437, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.8067 - accuracy: 0.7916 - val_loss: 1.5087 - val_accuracy: 0.6511\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.54437 to 1.50873, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.7216 - accuracy: 0.8173 - val_loss: 1.4990 - val_accuracy: 0.6611\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.50873 to 1.49901, saving model to best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.6605 - accuracy: 0.8289 - val_loss: 1.5350 - val_accuracy: 0.6511\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.49901\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 0.6584 - accuracy: 0.8268 - val_loss: 1.5723 - val_accuracy: 0.6661\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.49901\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.6537 - accuracy: 0.8303 - val_loss: 1.5891 - val_accuracy: 0.6444\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.49901\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.6090 - accuracy: 0.8422 - val_loss: 1.5496 - val_accuracy: 0.6650\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.49901\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 1s 61ms/step - loss: 0.5342 - accuracy: 0.8655 - val_loss: 1.5995 - val_accuracy: 0.6722\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.49901\n",
      "Epoch 00031: early stopping\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=50  \n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)  \n",
    "mc = ModelCheckpoint('best', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    callbacks=(es,mc),\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, acc, 'red', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'green', label='Validation acc')\n",
    "plt.plot(epochs, loss, 'purple', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'blue', label='Validation loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 마치며"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 루브릭\n",
    "***\n",
    "\n",
    "|**평가문항**|**상세기준**|\n",
    "|------------|-------------|\n",
    "|1. 분류 모델의 accuracy가 기준 이상 높게 나왔는가?|3가지 단어 개수에 대해 8가지 머신러닝 기법을 적용하여 그중 최적의 솔루션을 도출하였다.|\n",
    "|2. 분류 모델의 F1 score가 기준 이상 높게 나왔는가?|Vocabulary size에 따른 각 머신러닝 모델의 성능변화 추이를 살피고, 해당 머신러닝 알고리즘의 특성에 근거해 원인을 분석하였다.|\n",
    "|3. 딥러닝 모델을 활용해 성능이 비교 및 확인되었는가?|동일한 데이터셋과 전처리 조건으로 딥러닝 모델의 성능과 비교하여 결과에 따른 원인을 분석하였다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 느낀점\n",
    "\n",
    "* 초반에 노드에서 배웠던 내용들을 좀 더 깊게 배우는 느낌이 들어 좋았다.\n",
    "* 처음 접했을 땐 상당히 어렵게 느껴지던 개념들이었는데, 두번쨰 보는거라고 거부감이 덜해 학습하면서 기분이 묘했다. 물론 여전히 깊게 이해하진 못했다.\n",
    "* 3가지 버전의 vocab size 로 인해 굉장히 볼륨이 많게 느껴진 프로젝트였다. 불륨 압박으로 제시된 모델 중 몇가지를 생략했다."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6d479fb901925bf95be03c57f66c8da4656e795ca75e28d88de06b246de9509"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('3.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
